# -*- coding: utf-8 -*-
"""FakeNewsLSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/Taokhan/4938b310f54b27e1363995ed7d49370e/fakenewslstm.ipynb
"""

import pandas as pd
import tensorflow as tf
import os
import re
import numpy as np
from string import punctuation
from zipfile import ZipFile
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Embedding, GRU, LSTM, RNN, SpatialDropout1D
from google.colab import drive

drive.mount('/content/gdrive')
root_path1 = 'gdrive/My Drive/ColapData/train.csv'
root_path2 = 'gdrive/My Drive/ColapData/test.csv'
train = pd.read_csv(root_path1)
test = pd.read_csv(root_path2)
train_data = train.copy()
test_data = test.copy()

train_data = train_data.set_index('id', drop = True)
train_data.head()

test_data.head()

train_data.isnull().sum()

train_data[['title', 'author']] = train_data[['title', 'author']].fillna(value = 'Missing')
train_data = train_data.dropna()
train_data.isnull().sum()

length = []
[length.append(len(str(text))) for text in train_data['text']]
train_data['length'] = length
train_data.head()

min(train_data['length']), max(train_data['length']), round(sum(train_data['length'])/len(train_data['length']))

len(train_data[train_data['length'] < 50])

train_data['text'][train_data['length'] < 50]

train_data = train_data.drop(train_data['text'][train_data['length'] < 50].index, axis = 0)

max_features = 1500
tokenizer = Tokenizer(num_words = max_features, filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n', lower = True, split = ' ')
tokenizer.fit_on_texts(texts = train_data['text'])
X = tokenizer.texts_to_sequences(texts = train_data['text'])

X = pad_sequences(sequences = X, maxlen = max_features, padding = 'pre')

print(X.shape)
y = train_data['label'].values
print(y.shape)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 101)

lstm_model = Sequential(name = 'lstm_nn_model')

lstm_model.add(layer = Embedding(input_dim = max_features, output_dim = 100, name = '1st_layer'))
lstm_model.add(layer = LSTM(units = 100, dropout = 0.1, recurrent_dropout = 0.1, name = '2nd_layer'))
lstm_model.add(layer = Dropout(rate = 0.5, name = '3rd_layer'))
lstm_model.add(layer = Dense(units = 100,  activation = 'relu', name = '4th_layer'))
lstm_model.add(layer = Dropout(rate = 0.5, name = '5th_layer'))
lstm_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])

lstm_model_fit = lstm_model.fit(X_train, y_train, epochs = 3)

test_data = test_data.set_index('id', drop = True)
test_data.shape

test_data = test_data.fillna(' ')
test_data.isnull().sum()

tokenizer.fit_on_texts(texts = test_data['text'])
test_text = tokenizer.texts_to_sequences(texts = test_data['text'])
test_text = pad_sequences(sequences = test_text, maxlen = max_features, padding = 'pre')

lstm_prediction = lstm_model.predict_classes(test_text)

result = pd.DataFrame({'id':test_data.index, 'label':lstm_prediction})

result.head()

